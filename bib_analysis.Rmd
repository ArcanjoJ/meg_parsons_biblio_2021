---
title: "bib_analysis"
output: html_document
---

# Workflow

- Clean DB

- Number of articles through time

- Which journals are being published in bar graph

- mean number of authors per articles

- Author collaboration network (tidygraph/tidygraph)

- Institution not always linked to country. George has code

- George has code to clean language special characters

- Finn did author initials cleaning code

- Keyword co-occurance (can split out by time)

- ngrams (whole abstract; tidytex)

- LDA topic within topic

- DO THIS BETORE TOPIC MODELLING Stopwords (throw out the, a, and ...) and lemitize (contractions), another that does run/ran (tidytex)

- revtools package



Check

- column outliers

- abstracts/keywords

- unrelated titles

- DOI (ISSN may be a bad record)

- Language (english)


```{r include=FALSE}
library(tidyverse)
# library(data.table)
library(janitor)
library(rscopus)
library(lubridate)
library(tidytext)
library(ggwordcloud)
# library(ggthemes)
library(tidylo)
library(widyr)
library(tidygraph)
library(ggraph)
library(patchwork)

facet_bar <- function(df, y, x, by, nrow = 2, ncol = 2, scales = "free") {
  mapping <- aes(y = reorder_within({{ y }}, {{ x }}, {{ by }}), 
                 x = {{ x }}, 
                 fill = {{ by }})
  
  facet <- facet_wrap(vars({{ by }}), 
                      nrow = nrow, 
                      ncol = ncol,
                      scales = scales) 
  
  ggplot(df, mapping = mapping) + 
    geom_col(show.legend = FALSE) + 
    scale_y_reordered() + 
    facet + 
    ylab("")
}
```


# Corpus overview

```{r include=FALSE}
# The following code requires on-campus network access. Work around for lockdown was to use a nectar VM. Using the UoA squid proxy should work as well. The data from the query have been downloaded and saved so the following code is commented out. 1827 results have been returned


# rscopus::set_api_key("cf71c76658efeaf1c2da32d64bc8e1d6")
# scopus_query <- scopus_search(query = "TITLE-ABS-KEY ( \"climate justice\"  OR  
#                        \"environmental justice\"  OR  \"social justice\"  AND  
#                        \"climate change\"  OR  \"Global warming\" )  AND  
#                        ( LIMIT-TO ( LANGUAGE ,  \"English\" ) )", 
#                        view = "COMPLETE", count = 25, max_count = 2500)
# 
# scopus_results <- gen_entries_to_df(res70$entries)
```


```{r include=FALSE}
# Some formatting/wrangling

scopus_results <- readRDS("data/scopus_api_1827.rds")
scopus_results <- lapply(scopus_results, clean_names)
str(scopus_results[[1]])
str(scopus_results$affiliation)
str(scopus_results$author)
str(scopus_results$`prism:isbn`)
lapply(scopus_results, str)

author_df <- scopus_results[[1]] |> 
  mutate(prism_cover_date = as_date(prism_cover_date),
         author_count_total = as.numeric(author_count_total),
         year = floor_date(prism_cover_date, "year"),
         prism_issn = as.numeric(prism_e_issn),
         prism_e_issn = as.numeric(prism_e_issn),
         prism_volume = as.numeric(prism_volume),
         citedby_count = as.numeric(citedby_count)
         )


author_df[which(is.na(author_df$subtype_description)),]

table(author_df$subtype, exclude = NULL) # only 1 undefined
table(author_df$subtype_description, exclude = NULL) # undefined shows as NA


sapply(author_df, function(x) sum(is.na(x)))

```


## What publication types (books, artilces...)

```{r echo=FALSE, warning=FALSE}
ggplot(author_df, aes(fct_infreq(subtype_description))) +
  geom_bar() +
  scale_x_discrete(na.translate = FALSE) + # Leave out the 1 NA
  theme_minimal()
```


## Top 20 most popular journals

```{r echo=FALSE}
# ggplot(author_df, aes(fct_infreq(prism_publication_name))) +
#   geom_bar() +
#   scale_x_discrete()

journal_df <- author_df |>
  count(prism_publication_name, sort = TRUE) |>
  mutate(prism_publication_name = fct_reorder(prism_publication_name, n))

# ggplot(tail(journal_df, 20), aes(x = prism_publication_name, y = n)) +
#   geom_bar(stat = "identity") +
#   scale_x_discrete(limits = rev, labels = scales::wrap_format(15)) +
#   theme(axis.text.x = element_text(angle = 45))

ggplot(head(journal_df, 20), aes(n, prism_publication_name)) +
  geom_col() +
  scale_y_discrete(labels = scales::wrap_format(20)) +
  labs(y = NULL) +
  theme_minimal()

```
Environmental Justice journal established in March 2008


## Publications through time

```{r echo=FALSE}
publications_by_year <- author_df |> 
  select(year) |>
  group_by(year) |> 
  summarise(count = length(year))

ggplot(publications_by_year, aes(x = year, y = count)) +
  geom_line() +
  scale_x_date(limits = c(publications_by_year$year[1], "2020-01-01")) +
  theme_minimal()

```


## Mean number of authors through time

```{r echo=FALSE}
mean_author_by_year <- author_df |> 
  select(year, author_count_total) |>
  group_by(year) |> 
  summarise(mean_auth = mean(author_count_total))

ggplot(mean_author_by_year, aes(x = year, y = mean_auth)) +
  geom_line() +
  theme_minimal()

```

## Most productive authors (first and not)

```{r echo=FALSE, message=FALSE}
dim(scopus_results$author) # 4137 rows
sum(is.na(scopus_results$author$given_name)) # 28 NA
sum(is.na(scopus_results$author$surname)) # 21 NA
scopus_results$author[which(is.na(scopus_results$author$given_name)),]


# Am I having issues with duplicate authors? I.e., with/without initials in first name?
productive_authors_df <- scopus_results$author |> 
  drop_na(surname) |> # Most productive author is NA NA lol
  unite("full_name", c("surname", "given_name"), sep = ", ", remove = FALSE) |> 
  count(full_name, sort = TRUE) |>
  mutate(full_name = fct_reorder(full_name, n))

# unite can also be: mutate(full_name = str_c(scopus_results$author$given_name, scopus_results$author$surname, sep = ", "))

ggplot(head(productive_authors_df, 20), aes(n, full_name)) +
  geom_col() +
  scale_y_discrete(labels = scales::wrap_format(15), na.translate = FALSE) +
  labs(y = NULL) +
  theme_minimal()

```


## Most cited papers

```{r echo=FALSE, message=FALSE}
sum(is.na(author_df$citedby_count))

author_df |>
  arrange(desc(citedby_count)) |> 
  slice(1:10) |>
  mutate(dc_title = fct_reorder(dc_title, citedby_count)) |> 
  ggplot(aes(y = dc_title, x = citedby_count)) +
  geom_col() +
  scale_y_discrete(labels = scales::wrap_format(20)) +
  labs(y = NULL) +
  theme_minimal()

# Alternate way of doing the same thing:
# title_df <- author_df |>
#   select(dc_title, citedby_count) |> 
#   arrange(desc(citedby_count)) |> 
#   slice(1:15) |>
#   mutate(dc_title = fct_reorder(dc_title, citedby_count))
# 
# ggplot(title_df, aes(y = dc_title, x = citedby_count)) +
#   geom_col() +
#   scale_y_discrete(limits = rev, labels = scales::wrap_format(15)) +
#   labs(y = NULL) +
#   theme_minimal()

```



# Corpus analysis

## Most common words in abstracts.
Not surprisingly they match the search terms

```{r, echo=FALSE, message=FALSE}
# Tutorial groups by book does not work well for bigrams
# https://bookdown.org/Maxine/tidy-text-mining/the-unnest-tokens-function.html
# Does tokenize contract?

tidy_abs <- author_df |>
  select(entry_number, dc_description) |> 
  # group_by(entry_number) |>  # All abstracts not comparing publications
  unnest_tokens(word, dc_description) |> 
  anti_join(stop_words) |> 
  ungroup()

tidy_abs |>
  count(word, sort = T) |>
  filter(n > 600) |>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL) +
  theme_minimal()

# Top n words
# tidy_abs |> 
#   count(word, sort = TRUE) |>
#   top_n(10) |> 
#   mutate(word = reorder(word, n)) |>
#   ggplot() +
#   geom_col(aes(x = n, y = word))

```

## Most common word pairs in abstracts (again, no surprises)

```{r, echo=FALSE, message=FALSE}

# bigrams does not work well grouped by entry (too little information in abs)
# tutorial does not group_by book https://bookdown.org/Maxine/tidy-text-mining/tokenizing-by-n-gram.html
tidy_abs_ngrams <- author_df |>
  select(entry_number, dc_description) |> 
  unnest_tokens(bigram, dc_description, token = "ngrams", n = 2) |> 
  separate(bigram, into = c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% c(stop_words$word, NA),
         !word2 %in% c(stop_words$word, NA)) |>
  unite(bigram, c(word1, word2), sep = " ") |>
  count(bigram, sort = T) 

tidy_abs_ngrams |>
  slice(1:20) |>
  mutate(bigram = reorder(bigram, n)) |>
  ggplot(aes(n, bigram)) +
  geom_col() +
  labs(y = NULL) +
  theme_minimal()

```

## Network of word pairs

Network graph of most commonly joined words. "Note that this is a visualization of a Markov chain, a common model in text processing, where the choice of a word only depends on its previous word".

```{r echo=FALSE}
tidy_abs_ngrams_net <- author_df |>
  select(entry_number, dc_description) |> 
  unnest_tokens(bigram, dc_description, token = "ngrams", n = 2) |> 
  separate(bigram, into = c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% c(stop_words$word, NA),
         !word2 %in% c(stop_words$word, NA)) |>
  unite(bigram, c(word1, word2), sep = " ", remove = FALSE) |>
  count(word1, word2, sort = T) 

bigram_graph <- tidy_abs_ngrams_net %>% 
  filter(n > 20) %>%
  as_tbl_graph()

arrow <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") + 
  geom_edge_link(aes(alpha = n), show.legend = F, 
                 arrow = arrow, end_cap = circle(0.07, "inches")) + 
  geom_node_point(color = "lightblue", size = 5) + 
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

# tf-idf and log ratio

Compares documents, not useful in this case but curious. Maybe can be analysed furter?

```{r}
# The following compares documents. Not useful in this case.
tf_idf <- author_df |>
  select(entry_number, dc_description) |>
  unnest_tokens(word, dc_description) |>
  add_count(entry_number, name = "total_words") |>
  group_by(entry_number, total_words) |>
  count(word, sort = TRUE) |>
  ungroup() |>
  select(-total_words) |>
  # mutate(all = "all_abs") |>
  bind_tf_idf(term = word, document = entry_number, n = n) |>
  bind_log_odds(set = entry_number, feature = word, n = n) |>
  arrange(desc(tf_idf))

```

A word cloud (because why not?)

```{r, echo=FALSE, message=FALSE}

tidy_abs |>
  count(word, sort = T) |> 
  top_n(200) |> 
  ggplot() +
  geom_text_wordcloud_area(aes(label = word, size = n)) +
  scale_size_area(max_size = 15) +
  theme_light()


```


# Correlating word pairs

```{r, echo=FALSE, message=TRUE}
tidy_abs_widyr <- tidy_abs |>
  pairwise_count(word, entry_number, sort = TRUE)

tidy_abs_widyr_word_cors <- tidy_abs |> 
  add_count(word) |> 
  filter(n >= 20) |> 
  select(-n) |>
  pairwise_cor(word, entry_number, sort = TRUE)

# Using search terms, issues with contractions?
tidy_abs_widyr_word_cors |>
  filter(item1 %in% c("climate", "change", "justice", "social",
                      "environmental", "global", "warming")) |>
  group_by(item1) |>
  top_n(6) |>
  ungroup() |>
  facet_bar(y = item2, x = correlation, by = item1, nrow = 7)

```


```{r, echo=FALSE, message=TRUE}
# problem with grouping variable same as bigrams
tidy_abs_widyr_word_cors %>%
  filter(correlation > .5) %>%
  as_tbl_graph() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE)

```




